embedding_size: 64
n_layers: 2
reg_weight: 1e-4

ssl_temp: 0.8
ssl_reg: 1e-7
hyper_layers: 1

alpha: 1

proto_reg: 8e-8
num_clusters: 100

m_step: 1
warm_up_step: 20

eval_step: 1
epochs: 10
train_batch_size: 500
eval_batch_size: 8196

######################

lambda: 0.1
eps: 0.2
temperature: 0.2
layer_cl: 1
require_pow: True

q: 5
